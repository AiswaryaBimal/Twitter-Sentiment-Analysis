{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# utilities\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# nltk\nfrom nltk.stem import WordNetLemmatizer\n\n# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the dataset\nimport pandas as pd\nDATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n\n# Removing the unnecessary columns.\ndataset = dataset[['sentiment','text']]\n# Replacing the values to ease understanding.\ndataset['sentiment'] = dataset['sentiment'].replace(4,2,0)\n\n# Plotting the distribution for dataset.\nax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax.set_xticklabels(['Negative','Neutral','Positive'], rotation=0)\n\n# Storing data in lists.\ntext, sentiment = list(dataset['text']), list(dataset['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\n## Defining set containing all stopwords in english.\nstopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n             \"youve\", 'your', 'yours', 'yourself', 'yourselves']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer.\n    wordLemm = WordNetLemmatizer()\n    \n    # Defining regex patterns.\n    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,' URL',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,' USER', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            #if word not in stopwordlist:\n            if len(word)>1:\n                # Lemmatizing the word.\n                word = wordLemm.lemmatize(word)\n                tweetwords += (word+' ')\n            \n        processedText.append(tweetwords)\n        \n    return processedText","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nt = time.time()\nprocessedtext = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word-Cloud for Negative tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_neg = processedtext[:800000]\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n               collocations=False).generate(\" \".join(data_neg))\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word-Cloud for Positive tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pos = processedtext[800000:]\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (20,20))\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n                                                    test_size = 0.05, random_state = 0)\nprint(f'Data Split done.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint(f'Vectoriser fitted.')\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)\nprint(f'Data Transformed.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Model Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a name=\"p8-1\">BernoulliNB Model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"BNBmodel = BernoulliNB(alpha = 2)\nBNBmodel.fit(X_train, y_train)\nmodel_Evaluate(BNBmodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a name=\"p8-2\">LinearSVC Model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"SVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nmodel_Evaluate(SVCmodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a name=\"p8-3\">Logistic Regression Model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nLRmodel.fit(X_train, y_train)\nmodel_Evaluate(LRmodel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = open('vectoriser-ngram-(1,2).pickle','wb')\npickle.dump(vectoriser, file)\nfile.close()\n\nfile = open('Sentiment-LR.pickle','wb')\npickle.dump(LRmodel, file)\nfile.close()\n\nfile = open('Sentiment-BNB.pickle','wb')\npickle.dump(BNBmodel, file)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_models():\n    '''\n    Replace '..path/' by the path of the saved models.\n    '''\n    \n    # Load the vectoriser.\n    file = open('..path/vectoriser-ngram-(1,2).pickle', 'rb')\n    vectoriser = pickle.load(file)\n    file.close()\n    # Load the LR Model.\n    file = open('..path/Sentiment-LRv1.pickle', 'rb')\n    LRmodel = pickle.load(file)\n    file.close()\n    \n    return vectoriser, LRmodel\n\ndef predict(vectoriser, model, text):\n    # Predict the sentiment\n    textdata = vectoriser.transform(preprocess(text))\n    sentiment = model.predict(textdata)\n    \n    # Make a list of text with sentiment.\n    data = []\n    for text, pred in zip(text, sentiment):\n        data.append((text,pred))\n        \n    # Convert the list into a Pandas DataFrame.\n    df = pd.DataFrame(data, columns = ['text','sentiment'])\n    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n    return df\n\nif __name__==\"__main__\":\n    # Loading the models.\n    #vectoriser, LRmodel = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"I hate twitter\",\n            \"May the Force be with you.\",\n            \"Mr. Stark, I don't feel so good\"]\n    \n    df = predict(vectoriser, LRmodel, text)\n    print(df.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}